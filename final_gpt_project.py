# -*- coding: utf-8 -*-
"""Final_GPT_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_8H67fles1ikvNrC2gRV3RGnGhNlZui
"""

import torch
import torch.nn as nn
from torch.nn import functional as F

with open('harrypotter1.txt', 'r', encoding='utf-8') as f:
     text = f.read()

print('length of dataset in chars: ', len(text))

# First 1000 chars
print(text[:1000])

# here are all the unique characters that occur in this text
chars = sorted(list(set(text)))
vocab_size = len(chars)
# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string
print(''.join(chars))
print(vocab_size)
print(encode('hii there'))
print(decode(encode('hii there')))

# Train and test splits TOKENIZE-ATION
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9*len(data)) # first 90% will be train, rest val
train_data = data[:n]
val_data = data[n:]
print(data.shape, data.dtype)
print(data[:1000])



